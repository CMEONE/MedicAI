{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nonDL_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnDIBA0s4VGv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "oDNikAZ_420K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('drive/MyDrive/June 24-25/mtsamples.csv')\n",
        "data.loc[data.medical_specialty == ' Cardiovascular / Pulmonary', \"medical_specialty\"] = 'Heart'\n",
        "data.loc[data.medical_specialty == ' Neurosurgery', 'medical_specialty'] = 'Brain'\n",
        "data.loc[data.medical_specialty == ' Neurology', 'medical_specialty'] = 'Brain'\n",
        "data.loc[data.medical_specialty == ' Urology', 'medical_specialty'] = 'Reproductive'\n",
        "data.loc[data.medical_specialty == ' Obstetrics / Gynecology', 'medical_specialty'] = 'Reproductive'\n",
        "data.loc[data.medical_specialty == ' Gastroenterology', 'medical_specialty'] = 'Digestive'\n",
        "data.loc[data.medical_specialty == ' Nephrology', 'medical_specialty'] = 'Digestive'\n",
        "data = data[data.medical_specialty.isin(['Heart', 'Brain', 'Reproductive', 'Digestive'])]\n",
        "data['medical_specialty'].value_counts()"
      ],
      "metadata": {
        "id": "ukYTH3Fy4a_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['transcription', 'medical_specialty']]\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "65KpMhDz4ki1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['medical_specialty'].value_counts()"
      ],
      "metadata": {
        "id": "fFQQs-3mFoUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.rename(columns = {'transcription':'Report', 'medical_specialty':'speciality'}, inplace = True)"
      ],
      "metadata": {
        "id": "Snj65Mi-wdyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "  text = text.lower()\n",
        "  text = text.strip()\n",
        "  text = re.compile('[/(){}\\[\\]\\|@,;]').sub(' ', text) \n",
        "  text = re.compile('[^0-9a-z #+_]').sub('', text) \n",
        "  words = text.split()\n",
        "  i = 0 \n",
        "  while i < len(words):\n",
        "    if words[i] in stopwords.words('english'):\n",
        "      words.pop(i)\n",
        "    else:\n",
        "      i += 1\n",
        "    \n",
        "    return ' '.join(map(str, words))\n",
        "\n",
        "def lemmatize(text):\n",
        "    wordlist=[]\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    sentences=sent_tokenize(text)\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        words=word_tokenize(sentence)\n",
        "        for word in words:\n",
        "            wordlist.append(lemmatizer.lemmatize(word))    \n",
        "    return ' '.join(wordlist) "
      ],
      "metadata": {
        "id": "JLfXwTzuFpch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['transcription'] = data['transcription'].apply(clean)\n",
        "data['transcription'] = data['transcription'].apply(lemmatize)"
      ],
      "metadata": {
        "id": "VIYo4SDVFqBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75, use_idf=True, smooth_idf=True, max_features=1000)\n",
        "tfIdfMat  = vectorizer.fit_transform(data['Report'].tolist() )\n",
        "feature_names = sorted(vectorizer.get_feature_names())\n",
        "print(feature_names)"
      ],
      "metadata": {
        "id": "xuPcsrCTISwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "pca = PCA(n_components=0.95)\n",
        "tfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())\n",
        "labels = data['speciality'].tolist()\n",
        "category_list = data.speciality.unique()\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels, test_size=0.2, random_state=42)   "
      ],
      "metadata": {
        "id": "WTQj49GlIfvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression().fit(X_train, y_train)\n",
        "clf = RandomForestClassifier().fit(X_train, y_train)\n",
        "y_test_pred= clf.predict(X_test)"
      ],
      "metadata": {
        "id": "xogZE7BeH2GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_test_pred,labels=category_list))"
      ],
      "metadata": {
        "id": "HfQWuqMdJRiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, y_test_pred)"
      ],
      "metadata": {
        "id": "cONaLr9tKWAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "tfIdfMatrix = tfIdfMat.todense()\n",
        "labels = data['speciality'].tolist()\n",
        "tsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(tfIdfMatrix)\n",
        "plt.figure(figsize=(16,10))\n",
        "palette = sns.hls_palette(4, l=.6, s=.9)\n",
        "sns.scatterplot(\n",
        "    x=tsne_results[:,0], y=tsne_results[:,1],\n",
        "    hue=labels,\n",
        "    palette= palette,\n",
        "    legend=\"full\",\n",
        "    alpha=0.3\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7c96YVxOSWKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(clf, 'LR.pkl')"
      ],
      "metadata": {
        "id": "WTCAp0Y9vwnl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}